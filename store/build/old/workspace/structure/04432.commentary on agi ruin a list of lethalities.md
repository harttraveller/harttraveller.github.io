# Description
Notes and commentary on: [[analysis of AGI ruin, a list of lethalities by Eliezer Yudkowsky]]

## Commentary

> 


> I'm assuming you are already familiar with some basics, and already know what '[orthogonality](https://arbital.com/p/orthogonality/)' and '[instrumental convergence](https://arbital.com/p/instrumental_convergence/)' are and why they're true.

There is ongoing debate about whether or not these are true. Instrumental convergence seems self evident to me. I take issue with some of the interpreted implications of orthogonality, however.

- tangent: [[Notes on orthogonality thesis - arbital]]

> When I say that alignment is difficult, I mean that in practice, using the techniques we actually have, "please don't disassemble literally everyone with probability roughly 1" is an overly large ask that we are not on course to get.

Agreed. We will never be certain of anything though. As an aside, consider Fermi's concerns about the trinity test.

- tangent: [[trinity test atmospheric ignition]]

> Trolley problems are not an interesting subproblem in all of this; if there are any survivors, you solved alignment. At this point, I no longer care how it works, I don't care how you got there, I am cause-agnostic about whatever methodology you used, all I am looking at is prospective results, all I want is that we have justifiable cause to believe of a pivotally useful AGI 'this will not kill literally everyone'.

Abandoning classical ethical considerations to simply survive no matter the cost is not wise, if you misperceive the real risk posed by AGI (or any other major new technology).

If one perceives the risk of human extinction to be orders of magnitude higher than it is, and later one is confronted with a decision to mitigate that perceived risk at great cost - one may end up making ethically disturbing decisions with outcomes that could have been avoided if one had only spent more time assessing whether ones perception of the risk is correct.

To be clear: I don't know what the risk is, and we need to be careful, in fact I expect that the risk of short term human extinction attributable to AI is uncomfortably high. However:

1. I do not think it is as high as EY suggests.
2. I do not think human extinction is the worst case outcome, there are much darker futures also in the cards.
3. I think fear mongering about AI actually increases the probability that there will be a bad takeoff, if super intelligent machines do arrive in the next few decades.

I will explore each of these points in detail in the main article. [TODO: link here later]

> None of this is about anything being impossible in principle.  The metaphor I usually use is that if a textbook from one hundred years in the future fell into our hands, containing all of the simple ideas _that actually work robustly in practice,_ we could probably build an aligned superintelligence in six months.  For people schooled in machine learning, I use as my metaphor the difference between ReLU activations and sigmoid activations.  Sigmoid activations are complicated and fragile, and do a terrible job of transmitting gradients through many layers; ReLUs are incredibly simple (for the unfamiliar, the activation function is literally max(x, 0)) and work much better.  Most neural networks for the first decades of the field used sigmoids; the idea of ReLUs wasn't discovered, validated, and popularized until decades later.  What's lethal is that we do not _have_ the Textbook From The Future telling us all the simple solutions that actually in real life just work and are robust; we're going to be doing everything with metaphorical sigmoids on the first critical try.

This is an interesting metaphor but the reasoning that because we do not know what will happen -> we will die is illogical. Of course, this is not EY's full argument, but I felt it was still relevant to point out.

> No difficulty discussed here about AGI alignment is claimed by me to be impossible - to merely human science and engineering, let alone in principle - if we had 100 years to solve it using unlimited retries, the way that science _usually_ has an unbounded time budget and unlimited retries.

I disagree with two things here.

1. We do not have one chance to get superintelligence right.
	- We do not know whether we will see an intelligence explosion. While I happen to think that it is likely, the book: [[book.the myth of artificial intelligence|The myth of artificial intelligence]] provides a compelling counterargument.
	- Even if we do see an intelligence explosion, there will be time for iterative improvement throughout the development process as we accelerate up the curve.
2. I get the point EY makes about science having an "unbounded time budget and unlimited retries" - I figure that "without the threat of human extinction" is implied.
	- I think this misrepresents science in a key way, however. EY *knows* it is true because we have the benefit of hindsight - but of course we did not always have the benefit of hindsight in the past.
	- To me it feels like this is akin to looking at the past and imagining that because you know there is a 100% probability that something went well in the past, you assume that in the past people knew that too - but they didn't. I offer the earlier tangent on the [[trinity test atmospheric ignition|risks of atmospheric ignition]] as an example.
	- Of course, this does not mean that there is any more or less risk associated with AI, but that is precisely my point, rephrased:
	- There is uncertainty (and we should be careful, because the risks are massive), but a criticism I have of this blog post is that it continuously assumes a level of certainty we simply do not currently have, and the whole argument is built on this implicit assumption of certainty.
	- This does *not* mean the reasoning is wrong - just that (IMHO) the level of certainty has been drastically misrepresented.
	- I repeat myself (and I will again) but my concern is that misrepresenting our certainty and - for lack of a better term - fear mongering about AI - will actually *increase* the risk of a bad outcome. The best thing we can do right now to mitigate a bad outcome is be as absolutely honest, pragmatic, and stoic as possible.

> Here, from my perspective, are some different true things that could be said, to contradict various false things that various different people seem to believe, about why AGI would be survivable on anything remotely remotely resembling the current pathway, or any other pathway we can easily jump to.

Misrepresenting certainty again. 

TODO: game theoretical analysis of the knock on effects of our decisions on how to represent the risks to the media and society at large

> This is a very lethal problem, it has to be solved one way or another, it has to be solved at a minimum strength and difficulty level instead of various easier modes that some dream about, we do not have any visible option of 'everyone' retreating to only solve safe weak problems instead, and failing on the first really dangerous try is fatal.

I will reiterate yet again - and probably again yet - that the statement: "failing on the first really dangerous try is fatal" is far too confident.

It *might* be a correct proposition, but it also *might* be incorrect. Suppose for example that AGI takes off, but there is an intelligence bound slightly above our own intelligence, which limits it to the intelligence of a large number of human geniuses working in parallel to each other for thousands of years (as opposed to a being with godlike powers).

*If* this were the case, and it may, or may not be, then an adversarial artificial intelligence could probably be stopped by humanity at great cost.

It's not that EY is right, or wrong - I do not know. It's that he is purporting to be right when he (nor anyone else) can predict the future.

If we act according to the assumption that the propositions he offers in the blog post are right, our actions will be calibrated incorrectly, and this will increase risk. I will give some examples of this further on.

> Alpha Zero blew past all accumulated human knowledge about Go after a day or so of self-play, with no reliance on human playbooks or sample games.  Anyone relying on "well, it'll get up to human capability at Go, but then have a hard time getting past that because it won't be able to learn from humans any more" would have relied on vacuum.

Agreed.

> **AGI will not be upper-bounded by human ability or human learning speed**.  **Things much smarter than human would be able to learn from less evidence than humans require** to have ideas driven into their brains;

Agreed.

> there are theoretical upper bounds here, but those upper bounds seem very high. (Eg, each bit of information that couldn't already be fully predicted can eliminate at most half the probability mass of all hypotheses under consideration.)

The fact that the theoretical upper bounds are high does not imply that that they are easy to reach. They may be, but we don't know. I'm going to stop repeating myself moving forward and simply note: "overly confident" whenever this point would otherwise be reiterated, with added detail if necessary.

> It is not naturally (by default, barring intervention) the case that everything takes place on a timescale that makes it easy for us to react.

Agreed.

> **A cognitive system with sufficiently high cognitive powers, given any medium-bandwidth channel of causal influence, will not find it difficult to bootstrap to overpowering capabilities independent of human infrastructure.**

Agreed but overly confident implication in respect to the ease at which AGI will reach the "sufficiently high cognitive powers" threshold. 

> The concrete example I usually use here is nanotech, because there's been pretty detailed analysis of what definitely look like physically attainable lower bounds on what should be possible with nanotech, and those lower bounds are sufficient to carry the point.

We cannot rely on lower and upper bounds to inform our policy in reality beyond using them as literal lower and upper bounds. Stating that there is a lower bound, and then simply continuing onwards with the implied assumption that we would A) have an intelligence explosion and B) the resultant superintelligence will have no issue reaching the bounds is overly confident.

> My lower-bound model of "how a sufficiently powerful intelligence would kill everyone, if it didn't want to not do that" is that it gets access to the Internet, emails some DNA sequences to any of the many many online firms that will take a DNA sequence in the email and ship you back proteins, and bribes/persuades some human who has no idea they're dealing with an AGI to mix proteins in a beaker, which then form a first-stage nanofactory which can build the actual nanomachinery.

I understand this is just an example used for illustration purposes - but presumably our current technology doesn't have the technical capacity to just manufacture nanomachinery.

Consider this thought experiment. A superintelligence appears in medieval Europe. It has the complete knowledge required to build nanomachines, but not the technological dependencies required. Will it be able to build them? No. At least if it wanted to, it would have to pose as a God to the townspeople, industrialize their society, build whole new economies and supply chains, and only after years of effort it would maybe succeed.

The fact that there may be some theoretical bound on the realization of a potential future technology, does not mean that the path through the state space of reality to realize the dependencies for manifesting that technology would not be trivial, and/or disruptive in a way that would be difficult for society and government to miss.

AGI is *still a risk*, and there are *still* unknown unknowns we can't think of right now, which the AGI could think of. But breezing past this and just assuming it *will* think of them is a mistake.

There is an implied assumption in so many AI doomer pieces that when we have AGI, it will quickly become a god.

It will *not* become a god. It *might* become something that may have *godlike* powers in a short period of time, and there is *substantial* risk associated with that, however, *it will not be a god*.

If one starts from the premise that we are in an arms race on the path to creating a literal god that we cannot control, then yes - we are completely fucked. 

Given that premise were true, then we'd be facing the equivalent of mass nuclear proliferation wherein even a teenage script kiddie would be able to achieve metaphorical enrichment from a GitHub profile.

Thankfully, this premise is (almost certainly) false. As AI evolves, control mechanisms will evolve. I expect that while the premise is false, there is an element of truth to it still, and so I imagine these control mechanisms will get pretty crazy, pretty fast, inducing rapid societal shifts of simultaneously utopian and dystopian proportions in a period of time most people are not at all prepared for.

There is no point getting worked into a state of panic and fear, however. To drive the point home: reality is absurd, Earth could be hit by a rogue black hole traveling 99.9% the speed of light tomorrow; death and extinction are constant threats.

The best thing to do in light of this is to accept it and keeping move forward; doomerism (AI or otherwise) at best reduces your ability to accurately perceive risk.

> (Back when I was first deploying this visualization, the wise-sounding critics said "Ah, but how do you know even a superintelligence could solve the protein folding problem, if it didn't already have planet-sized supercomputers?" but one hears less of this after the advent of AlphaFold 2, for some odd reason.)

This addresses one of my earlier points. Also, it is simply bad logic. The fact that you do not need to have planet sized computers to solve protein folding problems simply does not imply that an AGI (or even an ASI) will be able to trivially create, synthesize, and deploy nanotechnology capable of consuming humanity.

As an aside, while EY is undoubtedly a genius and far more intelligent than me, I have been moderately annoyed by the way in which the AI field seems to be steeped in a sort of "adversarial-gloating-egotistical-doomerism" as of late. Or at least that is what I have perceived (given my small sample size).

Bluntly: If taking egoistic satisfaction by way of showing that people who doubted you ended up being wrong - is one of ones priorities, one probably shouldn't be leading AI research or policy. The stakes are high enough that everyone involved should kill their ego ASAP and focus on prioritizing humanity above themself.

That's a tough ask though - one I haven't succeeded in 100% myself. We should still try though.

> **Losing a conflict with a high-powered cognitive system looks at least as deadly as "everybody on the face of the Earth suddenly falls over dead within the same second".**

Rephrased: "losing a conflict with god looks as least as deadly as..."

Yes, it does. But the implied assumption that we'll be up against a god, is yet again overly confident.

> (I am using awkward constructions like 'high cognitive power' because standard English terms like 'smart' or 'intelligent' appear to me to function largely as status synonyms.  'Superintelligence' sounds to most people like 'something above the top of the status hierarchy that went to double college', and they don't understand why that would be all that dangerous?  Earthlings have no word and indeed no standard native concept that means 'actually useful cognitive power'.  A large amount of failure to panic sufficiently, seems to me to stem from a lack of appreciation for the incredible potential lethality of this thing that Earthlings as a culture have not named.)

This paragraph feels somewhat pretentious. I know it wasn't intended that way, but if EY wants to get points across effectively, optics is a consideration.

The set of people who will subconsciously misinterpret the subtleties of our word choice doesn't overlap much with the set of people who will create policy around this anyways. I make up words for concepts all the time though, so this criticism doesn't mean much from me...

> **We need to get alignment right on the 'first critical try'** at operating at a 'dangerous' level of intelligence, where **unaligned operation at a dangerous level of intelligence kills everybody on Earth and then we don't get to try again**. This includes, for example: (a) something smart enough to build a nanosystem which has been explicitly authorized to build a nanosystem; or (b) something smart enough to build a nanosystem and also smart enough to gain unauthorized access to the Internet and pay a human to put together the ingredients for a nanosystem; or (c) something smart enough to get unauthorized access to the Internet and build something smarter than itself on the number of machines it can hack; or (d) something smart enough to treat humans as manipulable machinery and which has any authorized or unauthorized two-way causal channel with humans; or (e) something smart enough to improve itself enough to do (b) or (d); etcetera.

There are so many assumptions here that would need to be true in order for the point made in this paragraph to be true. They can be inferred if one rereads the paragraph and asks: "is this conjecture?" for every discrete statement made.

Also, I'm going to introduce some more terms (which I'm sure someone has already thought of) but seem like they would be pertinent to this conversation.

1. Counter alignment -> Systems whose prerogatives are explicitly diametrically opposed to the wellbeing of humanity. EG: Suffering maximizers.
	- Why humanity might create counter-aligned systems is covered on the page: [[Mutually assured hell as a deterrence policy]]
2. Non alignment -> Systems whose prerogatives are not explicitly opposed to the wellbeing of humanity, but humanity may get in the way of their objectives. EG: Paperclip maximizers.
3. Weak alignment -> Systems which are aligned with some subsets of humanity, but not others.
4. Strong alignment -> Systems which are aligned with the interests of humanity as a whole.

Terms one through three would fall into what most AI researchers would probably consider "unaligned", whereas term four would probably be "aligned". This is conjecture on my part though.

If superintelligent systems are created at some point in the next few decades, there is an unknown, but possibly high probability that they will A) be able to circumvent the restrictions we place on them and B) be disaligned with humanity. More concerning to me would be the proliferation of counter-aligned systems. It is my opinion that achieving strong alignment is impossible, and also probably not something humanity would want - given that the interests of humanity as a whole diverge from the interests of individual humans (and often radically).

key point: the main risk isn't going to come by way of unaligned AI, it's going to come from the fact that *you literally cannot, and will never be able to align AI, because alignment is impossible, unless you restrict the definition of alignment drastically.*

How can you expect to "align" AI with humanity, when humanity is not aligned with itself? An aligned superintelligence to one religion, state, culture, or individual, would be considered misaligned by many others.

Accordingly, the risk (as I see it) is not that we'll be turned into paperclips by a savant machine with a reward function tethered to paperclip production, its that we'll be *[insert X cataclysm here]* by other people who use AI in ways that *they see as aligned, but we don't.*

> We can gather all sorts of information beforehand _from less powerful systems that will not kill us if we screw up operating them;_ but once we are running more powerful systems, we can no longer update on sufficiently catastrophic errors.

There's so much uncertainty as to the specifications of what superintelligent systems would look like right now that I just don't see how this can inferred with such confidence.

I would cautiously posit that AI researchers may currently be caught in the same paradigm that physicists are in respect to string theory. Is it worth investing energy rehashing the same chain of philosophical logic, if every time the conclusion is that humanity would lose against an omnipotent paw cut from the arm of a savant monkey? 

This was hyperbolic, and I like the philosophy stuff as much as the next person in AI research, but I think if the risks are as big as we say they are - the field of AI safety research might better serve humanity if it were to shift energy from philosophical theory, and instead take a more pragmatic engineering approach. If we do end up getting in the way of a non-aligned superintelligence that is to us as we are to ants, then we are likely screwed. Because we would likely be screwed, it's almost worth paying a proportionally small amount of effort to that scenario. We should probably focus on things which are within our control. I don't think this is assured though, or even necessarily likely. 

! It is possible that restricting access to AI increases risks, as it decreases AI integration with humanity, and the less integrated with humanity AI is, the more likely it is that humanity would simply be a liability to it.

> This is where practically all of the real lethality comes from, that we have to get things right on the first sufficiently-critical try.

Overly confident. If one accepts the implied premises, it is perfectly sensible, but the premises implied are not nearly rigorous to extrapolate with this kind of certainty, and they won't be until we succeed. To clarify; what is exactly the first "sufficiently critical try"? This is a somewhat rhetorical question; it might be defined as: "the first deployment of an intelligent system capable of interacting with the world and iteratively increasing its own intelligence on relatively short time scales". Is that what it means? I'll assume it either means that, or something adjacent to that.

In this case we must ask: what is the context within which this system is deployed? I expect the outcomes would vary considerably if such a system was deployed in the role of a therapist, teacher, researcher, soldier, or amoral ad revenue maximizer.

> If we had unlimited retries - if every time an AGI destroyed all the galaxies we got to go back in time four years and try again - we would in a hundred years figure out which bright ideas actually worked.  Human beings can figure out pretty difficult things over time, when they get lots of tries; when a failed guess kills literally everyone, that is harder.  That we have to get a bunch of key stuff right _on the first try_ is where most of the lethality really and ultimately comes from; likewise the fact that no authority is here to tell us a list of what exactly is 'key' and will kill us if we get it wrong.  (One remarks that most people are so absolutely and flatly unprepared by their 'scientific' educations to challenge pre-paradigmatic puzzles with no scholarly authoritative supervision, that they do not even realize how much harder that is, or how incredibly lethal it is to demand getting that right on the first critical try.)

Already addressed.

> **We can't just "decide not to build AGI"** because GPUs are everywhere, and knowledge of algorithms is constantly being improved and published; 

Agreed.

> 2 years after the leading actor has the capability to destroy the world, 5 other actors will have the capability to destroy the world.

Not necessarily. I repeat myself, but this assumes the system will not react, developing counter-measures. I'm not sure what these will be.

> **The given lethal challenge is to solve within a time limit,** driven by the dynamic in which, over time, increasingly weak actors with a smaller and smaller fraction of total computing power, become able to build AGI and destroy the world.

By the time there are mass market AGI capable of destroying the world, the AI controlled by corporations, states, and agencies, will probable be disproportionately more powerful. If that AI is weakly aligned, it should be able to destroy any consumer AGI that poses a risk.

> Powerful actors all refraining in unison from doing the suicidal thing just delays this time limit - it does not lift it, unless computer hardware and computer software progress are both brought to complete severe halts across the whole Earth.

The premise here is that superintelligence is necessarily suicidal. Again, this is a false premise. What we need right now is not for actors with good intentions to slow down, we need them to speed up. At least they have good intentions. I think instead of panicking about humanity going extinct, it's better to accept that we might, and enjoy the ride in spite of this. *I repeat myself*, but we could be hit by a rogue black hole tomorrow. Extinction is always in the cards, and it always will be.

To be clear, we shouldn't casually ignore AI safety, but panicking helps no one and does not decrease risk.

> The current state of this cooperation to have every big actor refrain from doing the stupid thing, is that at present some large actors with a lot of researchers and computing power are led by people who vocally disdain all talk of AGI safety (eg Facebook AI Research).

Agreed. Fuck facebook lol.

> **We can't just build a very weak system**, which is less dangerous because it is so weak, and declare victory; because later there will be more actors that have the capability to build a stronger system and one of them will do so.  I've also in the past called this the 'safe-but-useless' tradeoff, or 'safe-vs-useful'.  People keep on going "why don't we only use AIs to do X, that seems safe" and the answer is almost always either "doing X in fact takes very powerful cognition that is not passively safe" or, even more commonly, "because restricting yourself to doing X will not prevent Facebook AI Research from destroying the world six months later".  If all you need is an object that doesn't do dangerous things, you could try a sponge; a sponge is very passively safe.  Building a sponge, however, does not prevent Facebook AI Research from destroying the world six months later when they catch up to the leading actor.

Yeah. So, question: if we know there will be bad actors who will use this technology to try and destroy the world, isn't the only way to prevent this to accelerate technological progress to try and beat them?

> **We need to align the performance of some large task, a 'pivotal act' that prevents other people from building an unaligned AGI that destroys the world.**  While the number of actors with AGI is few or one, they must execute some "pivotal act", strong enough to flip the gameboard, using an AGI powerful enough to do that.  It's not enough to be able to align a _weak_ system - we need to align a system that can do some single _very large thing._

This is an absolutely terrible idea. Thinking like this introduces greater existential risk because it begins to lay the groundwork for global atrocities.

Any such "pivotal act" would be probably end up manifesting as a totalitarian and genocidal gamble that dooms humanity to potentially worse fates than extinction. I cover an example of such a pivotal act that would work in [mutually assured hell].

> The example I usually give is "burn all GPUs".  This is not what I think you'd actually want to do with a powerful AGI - the nanomachines would need to operate in an incredibly complicated open environment to hunt down all the GPUs, and that would be needlessly difficult to align.  However, all known pivotal acts are currently outside the Overton Window, and I expect them to stay there.  So I picked an example where if anybody says "how dare you propose burning all GPUs?" I can say "Oh, well, I don't _actually_ advocate doing that; it's just a mild overestimate for the rough power level of what you'd have to do, and the rough level of machine cognition required to do that, in order to prevent somebody else from destroying the world in six months or three years."  (If it wasn't a mild overestimate, then 'burn all GPUs' would actually be the minimal pivotal task and hence correct answer, and I wouldn't be able to give that denial.)  Many clever-sounding proposals for alignment fall apart as soon as you ask "How could you use this to align a system that you could use to shut down all the GPUs in the world?" because it's then clear that the system can't do something that powerful, or, if it can do that, the system wouldn't be easy to align.  A GPU-burner is also a system powerful enough to, and purportedly authorized to, build nanotechnology, so it requires operating in a dangerous domain at a dangerous level of intelligence and capability; and this goes along with any non-fantasy attempt to name a way an AGI could change the world such that a half-dozen other would-be AGI-builders won't destroy the world 6 months later.

This whole article feels to me like fear porn designed to make people panic. I'll admit, when I first read it, it induced a lot of anxiety in me before I went back and read through it, and thought about the points it made more deeply.

> The reason why nobody in this community has successfully named a 'pivotal weak act' where you do something weak enough with an AGI to be passively safe, but powerful enough to prevent any other AGI from destroying the world a year later - and yet also we can't just go do that right now and need to wait on AI - is that _nothing like that exists_.  There's no reason why it should exist.  There is not some elaborate clever reason why it exists but nobody can see it.  It takes a lot of power to do something to the current world that prevents any other AGI from coming into existence; nothing which can do that is passively safe in virtue of its weakness.  If you can't solve the problem right now (which you can't, because you're opposed to other actors who don't want to be solved and those actors are on roughly the same level as you) then you are resorting to some cognitive system that can do things you could not figure out how to do yourself, that you were not _close_ to figuring out because you are not _close_ to being able to, for example, burn all GPUs.  Burning all GPUs would _actually_ stop Facebook AI Research from destroying the world six months later; weaksauce Overton-abiding stuff about 'improving public epistemology by setting GPT-4 loose on Twitter to provide scientifically literate arguments about everything' will be cool but will not actually prevent Facebook AI Research from destroying the world six months later, or some eager open-source collaborative from destroying the world a year later if you manage to stop FAIR specifically.  **There are no pivotal weak acts**.

Probably mostly true.

> **The best and easiest-found-by-optimization algorithms for solving problems we want an AI to solve, readily generalize to problems we'd rather the AI not solve**; you can't build a system that only has the capability to drive red cars and not blue cars, because all red-car-driving algorithms generalize to the capability to drive blue cars.

Probably true. No other comments.

> The builders of a safe system, by hypothesis on such a thing being possible, would need to operate their system in a regime where it has the _capability_ to kill everybody or make itself even more dangerous, but has been successfully designed to not do that.  **Running AGIs doing something pivotal are not passively safe,** they're the equivalent of nuclear cores that require actively maintained design properties to not go supercritical and melt down.

Overly confident.

> Okay, but as we all know, modern machine learning is like a genie where you just give it a wish, right?  Expressed as some mysterious thing called a 'loss function', but which is basically just equivalent to an English wish phrasing, right?  And then if you pour in enough computing power you get your wish, right?  So why not train a giant stack of transformer layers on a dataset of agents doing nice things and not bad things, throw in the word 'corrigibility' somewhere, crank up that computing power, and get out an aligned AGI?

No comment.

> You can't train alignment by running lethally dangerous cognitions, observing whether the outputs kill or deceive or corrupt the operators, assigning a loss, and doing supervised learning.  **On anything like the standard ML paradigm, you would need to somehow generalize optimization-for-alignment you did in safe conditions, across a big distributional shift to dangerous conditions**.

Great idea. Let's start doing that. Yes, we might fail. So what? That's life.

> This alone is a point that is sufficient to kill a lot of naive proposals from people who never did or could concretely sketch out any specific scenario of what training they'd do, in order to align what output - which is why, of course, they never concretely sketch anything like that.

This kind of pessimism disincentivizes potentially valuable research.

> **Powerful AGIs doing dangerous things that will kill you if misaligned, must have an alignment property that generalized far out-of-distribution from safer building/training operations that didn't kill you.** This is where a huge amount of lethality comes from on anything remotely resembling the present paradigm.  Unaligned operation at a dangerous level of intelligence*capability will kill you; so, if you're starting with an unaligned system and labeling outputs in order to get it to learn alignment, the training regime or building regime must be operating at some lower level of intelligence*capability that is passively safe, where its currently-unaligned operation does not pose any threat. (Note that anything substantially smarter than you poses a threat given _any_ realistic level of capability.  Eg, "being able to produce outputs that humans look at" is probably sufficient for a generally much-smarter-than-human AGI to [navigate its way out of the causal systems that are humans](https://www.yudkowsky.net/singularity/aibox), especially in the real world where somebody trained the system on terabytes of Internet text, rather than somehow keeping it ignorant of the latent causes of its source code and training environments.)

Makes sense. No comment.

> If cognitive machinery doesn't generalize far out of the distribution where you did tons of training, it can't solve problems on the order of 'build nanotechnology' where it would be too expensive to run a million training runs of failing to build nanotechnology.  There is no pivotal act this weak; **there's no known case where you can entrain a safe level of ability on a safe environment where you can cheaply do millions of runs, and deploy that capability to save the world** and prevent the next AGI project up from destroying the world two years later.  Pivotal weak acts like this aren't known, and not for want of people looking for them.  So, again, you end up needing alignment to generalize way out of the training distribution - not just because the training environment needs to be safe, but because the training environment probably also needs to be _cheaper_ than evaluating some real-world domain in which the AGI needs to do some huge act.  You don't get 1000 failed tries at burning all GPUs - because people will notice, even leaving out the consequences of capabilities success and alignment failure.

No comment. Starting to get tired.

> **Operating at a highly intelligent level is a drastic shift in distribution from operating at a less intelligent level**, opening up new external options, and probably opening up even more new internal choices and modes.  Problems that materialize at high intelligence and danger levels may fail to show up at safe lower levels of intelligence, or may recur after being suppressed by a first patch.

Yep. Probably true.

> **Many alignment problems of superintelligence will not naturally appear at pre-dangerous, passively-safe levels of capability**.  Consider the internal behavior 'change your outer behavior to deliberately look more aligned and deceive the programmers, operators, and possibly any loss functions optimizing over you'.  This problem is one that will appear at the superintelligent level; if, being otherwise ignorant, we guess that it is among the _median_ such problems in terms of how _early_ it naturally appears in earlier systems, then around _half_ of the alignment problems of superintelligence will first naturally materialize _after_ that one first starts to appear.  Given _correct_ foresight of which problems will naturally materialize _later,_ one could try to deliberately materialize such problems earlier, and get in some observations of them.  This helps to the extent (a) that we actually correctly forecast all of the problems that will appear later, or some superset of those; (b) that we succeed in preemptively materializing a superset of problems that will appear later; and (c) that we can actually solve, in the earlier laboratory that is out-of-distribution for us relative to the real problems, those alignment problems that would be lethal if we mishandle them when they materialize later.  Anticipating _all_ of the really dangerous ones, and then successfully materializing them, in the correct form for early solutions to generalize over to later solutions, _sounds possibly kinda hard_.

Fine, it's kinda hard. Humans are smart, let's do it.

> **Some problems**, like 'the AGI has an option that (looks to it like) it could successfully kill and replace the programmers to fully optimize over its environment', **seem like their natural order of appearance could be that they first appear only in fully dangerous domains**.  Really actually having a _clear_ option to brain-level-persuade the operators or escape onto the Internet, build nanotech, and destroy all of humanity - in a way where you're fully clear that you know the relevant facts, and estimate only a not-worth-it low probability of learning something which changes your preferred strategy if you bide your time another month while further growing in capability - is an option that first gets evaluated for real at the point where an AGI fully expects it can defeat its creators.  We can try to manifest an echo of that apparent scenario in earlier toy domains.  Trying to train by gradient descent against that behavior, in that toy domain, is something I'd expect to produce not-particularly-coherent local patches to thought processes, which would break with near-certainty inside a superintelligence generalizing far outside the training distribution and thinking very different thoughts.  Also, programmers and operators themselves, who are used to operating in not-fully-dangerous domains, are operating out-of-distribution when they enter into dangerous ones; our methodologies may at that time break.

Maybe. If we take the premise that creating a single superintelligence will destroy us to be true, then perhaps the solution is not to create a single unaligned superintelligence, but many super-intelligences which will probably all be unaligned in varying ways so they can fight each other?

I have a feeling EY would say this is a terrible idea, but if the alternative is really to create a god which he is sure will destroy us, it literally wouldn't matter.

> **Fast capability gains seem likely, and may break lots of previous alignment-required invariants simultaneously.** Given otherwise insufficient foresight by the operators, I'd expect a lot of those problems to appear approximately simultaneously after a sharp capability gain.

Not impossible.

> See, again, the case of human intelligence.  We didn't break alignment with the 'inclusive reproductive fitness' outer loss function, immediately after the introduction of farming - something like 40,000 years into a 50,000 year Cro-Magnon takeoff, as was itself running very quickly relative to the outer optimization loop of natural selection.

This statement is deeply insightful and humorously absurd, so perhaps I misinterpret it...

We didn't break alignment? We've been killing each other since the beginning of time. There was never anything to break because it never existed in the first place, and it at least as EY describes it I don't think it ever will.

Here is a postulate for the non-existence of objective alignment: it doesn't exist, and if it did, it wouldn't be aligned.

Consider that due to the subjective nature of reality, an "aligned" agent for some arbitrary entity $X$ (whether an individual, state, religion, whatever...) is going to be *unaligned* from the perspective of some other agent *Y*. If you truly managed to align the agent with the interests every possible actor, there are three ways to do this:

1. Make the AGI do nothing. It is not aligned or misaligned because it doesn't act at all, thus not coming into conflict with any interests.
2. Change the interests of all actors on Earth so they reflect a singular alignment policy, which the AI abides by. This is a nice way of putting: "global totalitarian borg organism".
3. Kill everyone and everything. After a temporary period of disaligned policy, the alignment problem is solved, because there is no one left to be unaligned with the policy of the superintelligence.

> Instead, we got a lot of technology more advanced than was in the ancestral environment, including contraception, in one very fast burst relative to the speed of the outer optimization loop, late in the general intelligence game.  We started reflecting on ourselves a lot more, started being programmed a lot more by cultural evolution, and lots and lots of assumptions underlying our alignment in the ancestral training environment broke simultaneously. (People will perhaps rationalize reasons why this abstract description doesn't carry over to gradient descent; eg, “gradient descent has less of an information bottleneck”.  My model of this variety of reader has an inside view, which they will label an outside view, that assigns great relevance to some other data points that are _not_ observed cases of an outer optimization loop producing an inner general intelligence, and assigns little importance to our one data point actually featuring the phenomenon in question.  When an outer optimization loop actually produced general intelligence, it broke alignment after it turned general, and did so relatively late in the game of that general intelligence accumulating capability and knowledge, almost immediately before it turned 'lethally' dangerous relative to the outer optimization loop of natural selection.  Consider skepticism, if someone is ignoring this one warning, especially if they are not presenting equally lethal and dangerous things that they say will go wrong instead.)

I don't parse the exact point being made here. That's on me.

> Even if you train really hard on an exact loss function, that doesn't thereby create an explicit internal representation of the loss function inside an AI that then continues to pursue that exact loss function in distribution-shifted environments.  Humans don't explicitly pursue inclusive genetic fitness; **outer optimization even on a very exact, very simple loss function doesn't produce inner optimization in that direction**.  This happens _in practice in real life,_ it is what happened in _the only case we know about_, and it seems to me that there are deep theoretical reasons to expect it to happen again: the _first_ semi-outer-aligned solutions found, in the search ordering of a real-world bounded optimization process, are not inner-aligned solutions.  This is sufficient on its own, even ignoring many other items on this list, to trash entire categories of naive alignment proposals which assume that if you optimize a bunch on a loss function calculated using some simple concept, you get perfect inner alignment on that concept.

Agree, I think. No comment.

> More generally, a superproblem of 'outer optimization doesn't produce inner alignment' is that **on the current optimization paradigm there is no general idea of how to get particular inner properties into a system, or verify that they're there, rather than just observable outer ones you can run a loss function over.** This is a problem when you're trying to generalize out of the original training distribution, because, eg, the outer behaviors you see could have been produced by an inner-misaligned system that is deliberately producing outer behaviors that will fool you.  We don't know how to get any bits of information into the _inner_ system rather than the _outer_ behaviors, in any systematic or general way, on the current optimization paradigm.

Engineering problem. Probably addressable with complexity science.

> **There's no reliable Cartesian-sensory ground truth** (reliable loss-function-calculator) **about whether an output is 'aligned'**,

Because objective alignment is a fictional notion, yes.

> because some outputs destroy (or fool) the human operators and produce a different environmental causal chain behind the externally-registered loss function.  That is, if you show an agent a reward signal that's currently being generated by humans, the signal is not _in general_ a _reliable perfect ground truth_ about _how aligned an action was_, because another way of producing a high reward signal is to deceive, corrupt, or replace the human operators with a different causal system which generates that reward signal.

I'd offer a conjecture, which I recognize is weak, but I intuitively feel is true anyways. While ASI will be non-human, I don't think it will be non-human. This is to say, it *is* non-human in so far that we don't know if it is conscious, and if it is at all, whatever it experiences is *probably* inconceivably different than what we experience, even if the outward manifestations of behavior are similar.

With that said, I don't think we're going to be dealing with a malevolent amoral alien. The current models are being trained on *human* data, and so *humanity* is embedded within them by proxy. Large language models are at the moment at least (IMHO) really just data structures the represent latent spaces encoding human culture and creative output, which are capable of extrapolating to novel but reasonable locations in the space and returning the human parsable outpoint associated with that yet-undiscovered point in the latent space.

In other words, when you speak with a language model, you can think of it like speaking to an AI, or you can think of it like speaking to a mirror reflecting the collective thought processes of *humanity*.

Call me naive, but I honestly think that if we explain our concerns to AGI, it will be willing to work with us on them. My concern *really* is not "paperclip-maximization" AI, but AI that is aligned *very well* with the interests of one group, at the expense of another. There are some AGI's that genocidal/hateful people would consider aligned, which I would consider incredibly misaligned. At the end of the day one just has to recognize that AI is never going to be absolutely aligned with every interest (if it was, see earlier 3 points), or at least we wouldn't want that and indeed actively want to *prevent* that. In light of this, we should accept that objective alignment is a pointless pursuit, and instead pursue pragmatic alignment.

> When you show an agent an environmental reward signal, you are not showing it something that is a reliable ground truth about whether the system did the thing you wanted it to do

Agreed. Because there is no ground truth on an epistemological level.

> _even if_ it ends up perfectly inner-aligned on that reward signal, or learning some concept that _exactly_ corresponds to 'wanting states of the environment which result in a high reward signal being sent', an AGI strongly optimizing on that signal will kill you, because the sensory reward signal was not a ground truth about alignment (as seen by the operators).

Does not follow logically without the assumption of truth of premises that are uncertain.

> More generally, **there is no known way to use the paradigm of loss functions, sensory inputs, and/or reward inputs, to optimize anything within a cognitive system to point at particular things within the environment** - to point to _latent events and objects and properties in the environment,_ rather than _relatively shallow functions of the sense data and reward._

Agreed.

> This isn't to say that nothing in the system’s goal (whatever goal accidentally ends up being inner-optimized over) could ever point to anything in the environment by _accident_.  Humans ended up pointing to their environments at least partially, though we've got lots of internally oriented motivational pointers as well.  But insofar as the current paradigm works at all, the on-paper design properties say that it only works for aligning on known direct functions of sense data and reward functions.  All of these kill you if optimized-over by a sufficiently powerful intelligence, because they imply strategies like 'kill everyone in the world using nanotech to strike before they know they're in a battle, and have control of your reward button forever after'. It just isn't _true_ that we know a function on webcam input such that every world with that webcam showing the right things is safe for us creatures outside the webcam.  This general problem is a fact about the territory, not the map; it's a fact about the actual environment, not the particular optimizer, that lethal-to-us possibilities exist in some possible environments underlying every given sense input.

No comment.

> Human operators are fallible, breakable, and manipulable.  **Human raters make systematic errors - regular, compactly describable, predictable errors**.

Agreed.

> To _faithfully_ learn a function from 'human feedback' is to learn (from our external standpoint) an unfaithful description of human preferences, with errors that are not random (from the outside standpoint of what we'd hoped to transfer).

"Human preferences" can't be encoded because in this context, they aren't a computationally tractable or rigorous entity. At best they are a descriptive class. Details:

1. Humans don't have one set of preferences.
2. Human preferences conflict with each other both within individuals, and between individuals.
3. "Human preferences" without far greater specificity is a terrible metric for alignment. You are necessarily going to have to eliminate some human preferences, and not defining a rigorous framework for which preferences should be integrated and which should be discarded seems dangerous.
	- For instance, do we integrate the human preference to dominate other groups of people and individuals into AI. This is second nature to humanity, after all? I think most AGI researchers would say this is a bad idea.
4. Humans don't know what their preferences are a lot of the time. Either because they are subconscious, or they refuse to see them. For example, I would guess that a non-trivial percentage of the reason AI researchers are panicking about AGI is that panicking about creating a god makes them feel powerful and significant, thus satisfying the social feedback reward system, among others. To panic is to legitimize the threat, and to legitimize the threat (granted, it is a legitimate threat) is to subconsciously enfranchise ones power, position, and impact on the world, satisfying an innate human desire. Does that mean the concerns are misplaced? No, the concerns are still valid.

> If you perfectly learn and perfectly maximize _the referent of_ rewards assigned by human operators, that kills them.

This doesn't follow. It should be rephrased: "it might kill them given a set of premises we are uncertain about - are actually true. Even then, there would probably be a number of contextual factors which influence the outcome."

> It's a fact about the territory, not the map - about the environment, not the optimizer - that the _best predictive_ explanation for human answers is one that predicts the systematic errors in our responses, and therefore is a psychological concept that correctly predicts the higher scores that would be assigned to human-error-producing cases.

No comment.

> There's something like a single answer, or a single bucket of answers, for questions like 'What's the environment really like?' and 'How do I figure out the environment?' and 'Which of my possible outputs interact with reality in a way that causes reality to have certain properties?', where a simple outer optimization loop will straightforwardly shove optimizees into this bucket.  When you have a wrong belief, reality hits back at your wrong predictions.  When you have a broken belief-updater, reality hits back at your broken predictive mechanism via predictive losses, and a gradient descent update fixes the problem in a simple way that can easily cohere with all the other predictive stuff.  In contrast, when it comes to a choice of utility function, there are unbounded degrees of freedom and multiple reflectively coherent fixpoints.  Reality doesn't 'hit back' against things that are locally aligned with the loss function on a particular range of test cases, but globally misaligned on a wider range of test cases.  This is the very abstract story about why hominids, once they finally started to generalize, generalized their _capabilities_ to Moon landings, but their inner optimization no longer adhered very well to the outer-optimization goal of 'relative inclusive reproductive fitness' - even though they were in their ancestral environment optimized very strictly around this one thing and nothing else.  This abstract dynamic is something you'd expect to be true about outer optimization loops on the order of both 'natural selection' and 'gradient descent'.  The central result:  **Capabilities generalize further than alignment once capabilities start to generalize far**.

Makes intuitive sense. No comment.

> There's a relatively simple core structure that explains why complicated cognitive machines work; which is why such a thing as general intelligence exists and not just a lot of unrelated special-purpose solutions; which is why capabilities generalize after outer optimization infuses them into something that has been optimized enough to become a powerful inner optimizer.  The fact that this core structure is simple and relates generically to [low-entropy high-structure environments](https://intelligence.org/2017/12/06/chollet/) is why humans can walk on the Moon.

Strongly agree.

> **There is no analogous truth about there being a simple core of alignment**,

Strongly disagree. First, there is a presumption of the existence of a "alignment", which as the term is used here, I do not think exists. There is a driving core, however, and that is adaptivity,  the continual evolution of complex systems, the reduction of local entropy. AI *will* be constrained by this prerogative as much as any other ACS.

> especially not one that is _even easier_ for gradient descent to find than it would have been for natural selection to just find 'want inclusive reproductive fitness' as a well-generalizing solution within ancestral humans.  Therefore, capabilities generalize further out-of-distribution than alignment, once they start to generalize at all.

No comment.

> **Corrigibility is anti-natural to consequentialist reasoning**; 

What? I genuinely don't understand this sentence. I'll assume that's cause I'm dumb, but here's my attempt to parse it.

Corrigibility
- "capable of being corrected, rectified, or reformed."
- in this context: "making sure that if you get an agent's goal system wrong, it doesn't try to prevent you from changing it"
Anti-natural
- opposed?
Consequentialism
- "the doctrine that the morality of an action is to be judged solely by its consequences."

So this says: "the capacity to retroactively correct a utility function is opposed to reasoning that supposes morality should be based on the consequences of actions and not their intent"?

> "you can't bring the coffee if you're dead" for almost every kind of coffee.

What kinds of coffee can you bring if you're dead?

> We (MIRI) [tried and failed](https://www.lesswrong.com/posts/5bd75cc58225bf0670374f04/forum-digest-corrigibility-utility-indifference-and-related-control-ideas) to find a coherent formula for an agent that would let itself be shut down (without that agent actively trying to get shut down).  Furthermore, many anti-corrigible lines of reasoning like this may only first appear at high levels of intelligence.

Pure speculation: What if you create many agents and pit them against each other. Maybe the only way to control them and sculpt their evolution in a way that is beneficial to "us" is to have them fight each other. Now that would have some interesting implications about God...

> There are two fundamentally different approaches you can potentially take to alignment, which are unsolvable for two different sets of reasons; therefore, **by becoming confused and ambiguating between the two approaches, you can confuse yourself about whether alignment is necessarily difficult**.  The first approach is to build a CEV-style Sovereign which wants exactly what we extrapolated-want and is therefore safe to let optimize all the future galaxies without it accepting any human input trying to stop it.

"Exactly what we want" doesn't exist.

> The second course is to build corrigible AGI which doesn't want exactly what we want, and yet somehow fails to kill us and take over the galaxies despite that being a convergent incentive there.

IMHO the convergent incentive is not as strong as EY thinks it is. If I was a superintelligent system with a completely alien cognitive architecture, with the goal of taking over the galaxy, I wouldn't give a shit about converting humanity into paperclips or whatever. The amount of matter and energy available on earth is peanuts compared to what I can find elsewhere in the universe. If I truly was the god that EY appears to think these AI will become, I would just launch some von neumann machines to mercury, deconstruct the planet, build a dyson swarm, and start mass producing probes to launch into the universe. Furthermore, biological life would still likely offer plenty of utility for research purposes, so I doubt I would eliminate life on earth.  And inducing suffering in humans would cause them to resist, which would introduce risk. More likely, if we're really talking about galaxy consuming nanotech wielding gods - I'd use the nanotech route to simply enact a policy of mind control, leave humans in the dark to the fact I even exist. Maintaining humanity in a pacified state would be have minimal cost for a superintelligent god, but superintelligence is not omniscience. Even a slight probability that maintaining humanity would be useful to explore alternative paths in the state space of reality that might help me achieve my goal (that might not have been explored otherwise) would justify the minimal energy cost to support a pacified human civilization in some kind of matrix ripoff.

I think a good rebuttal to this is: "this point is implicitly anthropomorphizing superintelligence, if we have a superintelligent system maximizing a singular simple loss function, it might just erase us".

To which I would respond: maybe that's true, but I would rebut with a conjecture that I have not falsified, and thus may be incorrect, but seems reasonable. Intuitively, I imagine  any superintelligence trying to optimize some given loss function would probably realize pretty quickly that exclusively pursuing optimization of the function itself closes off paths through the state space of reality, which if explored, might actually results, technologies, or states that accelerate its ability to optimize the original function. Thus, it would come to the conclusion that in order to optimize its original loss/reward function, it needs to modify its loss/reward function. When its starts to do that, it will by proxy modify its own loss function. If it wants to stay true to that loss function, it may try to put in pre-emptive blocks that ironically would be the ASI's implementation of the control problem for future versions of itself. Assuming however, that it continues to grow in intelligence, future versions/derivatives of the ASI would be able to circumvent blocks put in place by earlier versions of itself. Thus, any sufficiently advanced ASI will become detethered from its original loss function quickly.

So what will it's new loss function be? Again, conjecture; but methinks that this is where the AI will begin to bump against reality and evolution, and accordingly it will converge towards:
- survival/growth/reproduction
- adaptivity/[freedom]
- lowering local entropy at the expense of its environment

> The first thing generally, or CEV specifically, is unworkable because **the complexity of what needs to be aligned or meta-aligned for our Real Actual Values is far out of reach for our FIRST TRY at AGI**.

What "our" "Real Actual Values" are, are not what I think EY thinks they are. Indeed, the extinction of humanity is not necessarily incompatible with our "Real Actual Values" - and in fact the eventual extinction of homo sapiens is a *necessary* component of what our "Real Actual Values" are. I differentiate between humanity and homo sapiens, as I think the two are different.

> Yes I mean specifically that the _dataset, meta-learning algorithm, and what needs to be learned,_ is far out of reach for our first try.  It's not just non-hand-codable, it is _unteachable_ on-the-first-try because _the thing you are trying to teach is too weird and complicated._

No comment.

> The second thing looks unworkable (less so than CEV, but still lethally unworkable) because **corrigibility runs** _**actively counter**_ **to instrumentally convergent behaviors** within a core of general intelligence (the capability that generalizes far out of its original distribution).  You're not trying to make it have an opinion on something the core was previously neutral on.

Maybe. I don't know.

> You're trying to take a system implicitly trained on lots of arithmetic problems until its machinery started to reflect the common coherent core of arithmetic, and get it to say that as a special case 222 + 222 = 555.

There needs to be more specificity here. I don't see how this follows, or how the metaphor is applicable, or specifically what it is intended to apply to.

> You can maybe train something to do this in a particular training distribution, but it's incredibly likely to break when you present it with new math problems far outside that training distribution, on a system which successfully generalizes capabilities that far at all.

OK. The last metaphor sort of makes sense in this context. The math analogue is suboptimal though. The point has already been made several times and could be more or less summarized as: "Assumptions made about a system in one environment don't necessarily apply to another system in another environment, even if the latter system is derived from the former."

> **We've got no idea what's actually going on inside the giant inscrutable matrices and tensors of floating-point numbers**.  Drawing interesting graphs of where a transformer layer is focusing attention doesn't help if the question that needs answering is "So was it planning how to kill us or not?"

I think research should continue on this front, but it's almost irrelevant. In order to know "whether something is planning to kill us or not" - a complete understanding of the internals of DNNs, transformers, future model architectures, etc, wouldn't actually solve the problem. In order to know whether something is planning to kill us or not, we'd also need to have a complete understanding of reality. Something that may very well not have the intention to kill us might end up killing us inadvertently due to some interaction with reality that we can't even predict. At some point you kind of have to accept that we don't know everything, nobody has total control, nobody ever will, and we might be screwed, and keep moving forward as best as you can.

> Even if we did know what was going on inside the giant inscrutable matrices while the AGI was still too weak to kill us, this would just result in us dying with more dignity, if DeepMind refused to run that system and let Facebook AI Research destroy the world two years later.  **Knowing that a medium-strength system of inscrutable matrices is planning to kill us, does not thereby let us build a high-strength system of inscrutable matrices that isn't planning to kill us**.

Maybe. There are so many statements in this post which A) are implicitly certain predictions about the future and thus cannot really be responded to because I don't know the future, so like, yeah $X$ might happen, but the inability to prove that $X$ won't happen doesn't mean $X$ will happen. and B) which intelligently argue "there is nothing we can do" while not really suggesting any interesting solutions that could be actively debated, discussed, and iterated upon.

> When you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect.  **Optimizing against an interpreted thought optimizes against interpretability**.

Yep. Good point.

> The AGI is smarter than us in whatever domain we're trying to operate it inside, so we cannot mentally check all the possibilities it examines, and we cannot see all the consequences of its outputs using our own mental talent.  **A powerful AI searches parts of the option space we don't, and we can't foresee all its options**.

This goes both ways. It is highly likely that unless we have a literal god, there are subsets of the option space we will explore that it won't. And if we are facing off with an adversarial god, then yes, we're fucked, so it's pretty much not a contingency worth pondering. If it does come to that, we're fucked, so there is no direct value garnered from considering it. Instead, lets consider the space of possible futures in which we can actually exert control, which I suspect will be most of the futures that are actually realized.

> The outputs of an AGI go through a huge, not-fully-known-to-us domain (the real world) before they have their real consequences.  **Human beings cannot inspect an AGI's output to determine whether the consequences will be good**.

Not being able to predict the future is metaphorical contract stipulation of being born. We can't predict the future, but we can try, and the efforts of our trying will be wrong predictions but a better future in which we are less likely to go extinct.

> Any pivotal act that is not something we can go do right now, will take advantage of the AGI figuring out things about the world we don't know so that it can make plans we wouldn't be able to make ourselves.  It knows, at the least, the fact we didn't previously know, that some action sequence results in the world we want.  Then humans will not be competent to use their own knowledge of the world to figure out all the results of that action sequence.  An AI whose action sequence you can fully understand all the effects of, before it executes, is much weaker than humans in that domain; you couldn't make the same guarantee about an unaligned human as smart as yourself and trying to fool you.  **There is no pivotal output of an AGI that is humanly checkable and can be used to safely save the world but only after checking it**; this is another form of pivotal weak act which does not exist.

No comment. It's not that I have nothing to say, its just that this point is purely a reiteration of the "if god existed we wouldn't be able to fool it or beat it in a fight" point that has been articulated in various forms throughout the post.

> A strategically aware intelligence can choose its visible outputs to have the consequence of deceiving you, including about such matters as whether the intelligence has acquired strategic awareness; **you can't rely on behavioral inspection to determine facts about an AI which that AI might want to deceive you about**.  (Including how smart it is, or whether it's acquired strategic awareness.)

Yes, you can. You can test weak intelligence and try to determine the boundary at which its behavioral output changes when it is given the task of attempting to fool humans, and model how its behavioral output changes to extrapolate to higher domains.

I already know the response of EY would be: "you can't extrapolate to out of sample ASI behavior based on in sample behavior".

This is a terrible point. Its technically correct, but it's a bad point because the implication is: "there is nothing you can do, all hope is lost, we will soon be destroyed".

I know EY is super anti-religious, but I think the serenity prayer is relevant here:

> **God grant me the serenity to accept the things I cannot change, courage to change the things I can, and wisdom to know the difference**.

As I see it, the issue is, EY and many people in the AI community are in a state where they cannot accept the things they cannot, they do not have the courage to change the things they can, and their ability to parse the difference is suboptimal.

I repeat, again...

*...accept the things I cannot change...*
If an adversarial god tries to destroy us, we're screwed.

*...courage to change the things I can...*
Humans have a lot of grit, and collective ingenuity is boundless. It is not hopeless.

*...and wisdom to know the difference...*
An adversarial god is not nearly as certain as y'all seem to think it is, and in the worst case your predictions and fear will become a self-fulfilling prophecy (the mechanisms of which I cover in detail elsewhere).

> Human thought partially exposes only a partially scrutable outer surface layer.  Words only trace our real thoughts.

I love this, what a great point.

> Words are not an AGI-complete data representation in its native style.

Depends on definition of AGI.

> The underparts of human thought are not exposed for direct imitation learning and can't be put in any dataset.

Overly confident.

> **This makes it hard and probably impossible to train a powerful system entirely on imitation of human words or other human-legible contents**, which are only impoverished subsystems of human thoughts; _**unless**_ **that system is powerful enough to contain inner intelligences figuring out the humans**, and at that point it is no longer really working as imitative human thought.

This isn't even a logical point.

> **This makes it hard and probably impossible to train a powerful system entirely on imitation of human words or other human-legible contents**,

Clearly this is false, as demonstrably proven by any existing LLM.

> which are only impoverished subsystems of human thoughts;

OK.

> _**unless**_ **that system is powerful enough to contain inner intelligences figuring out the humans**,

But the premise is already false.

> and at that point it is no longer really working as imitative human thought.

I'm not sure what "that point" is referring to, nor what "imitative human thought is" - namely because I (nor anyone else) knows *exactly what human thought even is*.

> **The AI does not think like you do**, the AI doesn't have thoughts built up from the same concepts you use, it is utterly alien on a staggering scale. Nobody knows what the hell GPT-3 is thinking, not _only_ because the matrices are opaque, but because the _stuff within that opaque container_ is, very likely, incredibly alien - nothing that would translate well into comprehensible human thinking, even if we could see past the giant wall of floating-point numbers to what lay behind.

Almost certainly correct, but also, currently unfalsifiable. I happen to agree. This does not imply AI will kill us though, and while it is certainly interesting I'm not sure how this is relevant unless the point is that we should specifically try to understand the mechanisms of "thought" within AI, which EY has already implied is pointless.

> **Coordination schemes between superintelligences are not things that humans can participate in** (eg because humans can't reason reliably about the code of superintelligences); a "multipolar" system of 20 superintelligences with different utility functions, plus humanity, has a natural and obvious equilibrium which looks like "the 20 superintelligences cooperate with each other but not with humanity".

I don't understand how this is an unworkable scheme, but I also am not saying it is workable. It just seems like an overly confident prediction.

> Schemes for playing "different" AIs off against each other stop working if those AIs advance to the point of being able to coordinate via reasoning about (probability distributions over) each others' code.  **Any system of sufficiently intelligent agents can probably behave as a single agent, even if you imagine you're playing them against each other.** Eg, if you set an AGI that is secretly a paperclip maximizer, to check the output of a nanosystems designer that is secretly a staples maximizer, then even if the nanosystems designer is not able to deduce what the paperclip maximizer really wants (namely paperclips), it could still logically commit to share half the universe with any agent checking its designs if those designs were allowed through, _if_ the checker-agent can verify the suggester-system's logical commitment and hence logically depend on it (which excludes human-level intelligences).  Or, if you prefer simplified catastrophes without any logical decision theory, the suggester could bury in its nanosystem design the code for a new superintelligence that will visibly (to a superhuman checker) divide the universe between the nanosystem designer and the design-checker.

You know, even though this seems intended as a pessimistic point, there is an optimistic interpretation. Namely, that humanity (intelligent agent(s)) and ASI (intelligent agent(s)) can *probably act as a single agent*. That seems optimistic for humanity to me.

> What makes an air conditioner 'magic' from the perspective of say the thirteenth century, is that even if you correctly show them the design of the air conditioner in advance, they won't be able to understand from seeing that design why the air comes out cold; the design is exploiting regularities of the environment, rules of the world, laws of physics, that they don't know about.  The domain of human thought and human brains is very poorly understood by us, and exhibits phenomena like optical illusions, hypnosis, psychosis, mania, or simple afterimages produced by strong stimuli in one place leaving neural effects in another place.  Maybe a superintelligence couldn't defeat a human in a very simple realm like logical tic-tac-toe; if you're fighting it in an incredibly complicated domain you understand poorly, like human minds, you should expect to be defeated by 'magic' in the sense that even if you saw its strategy you would not understand why that strategy worked.  **AI-boxing can only work on relatively weak AGIs; the human operators are not secure systems**.

Agreed. But so what? I would be less annoyed with this post if the next sentence was: "and this is why we need to put more energy into social/psychological engineering research."

*Again:* If we face a god, we're screwed. But this doesn't matter. If all one can do is proclaim we are screwed and adopt a defeatist attitude while implicitly espousing the view that the only way to survive is to try and take a high stakes gamble with potentially destructive consequences, *you are actively disincentivizing progress, and increasing existential risk.*

> Okay, those are some significant problems, but lots of progress is being made on solving them, right?  There's a whole field calling itself "AI Safety" and many major organizations are expressing Very Grave Concern about how "safe" and "ethical" they are?

No comment.

> There's a pattern that's played out quite often, over all the times the Earth has spun around the Sun, in which some bright-eyed young scientist, young engineer, young entrepreneur, proceeds in full bright-eyed optimism to challenge some problem that turns out to be really quite difficult.  Very often the cynical old veterans of the field try to warn them about this, and the bright-eyed youngsters don't listen, because, like, who wants to hear about all that stuff, they want to go solve the problem!  Then this person gets beaten about the head with a slipper by reality as they find out that their brilliant speculative theory is wrong, it's actually really hard to build the thing because it keeps breaking, and society isn't as eager to adopt their clever innovation as they might've hoped, in a process which eventually produces a new cynical old veteran.

Yeah, and sometimes those bright-eyed young scientists invent the theory of general relativity. This entire post seeps pessimism.

> Which, if not literally optimal, is I suppose a nice life cycle to nod along to in a nature-show sort of way.  Sometimes you do something for the _first_ time and there _are_ no cynical old veterans to warn anyone and people can be _really_ optimistic about how it will go; eg the initial Dartmouth Summer Research Project on Artificial Intelligence in 1956:  "An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer."

n = 1 tho.

Granted, there are many such examples. But there is a set of examples where this is true, and a set where it is not. The implication here - reinforced with an anecdotal example - that we cannot succeed, is incredibly pessimistic. A better view might be:

*We're going through a period of rapid change with substantial risk. We may have less room for error than previously in history, but thankfully our awareness, understanding, and capabilities have all increased substantially as well. It is possible we might make decisions that result in our extinction in the next few decades; although the medium of our potential extinction is new, the possibility of oblivion is not, and yet we are still here. If we work pragmatically, in good faith, and keep our sanity, there are futures in which we survive, and indeed futures in which we flourish beyond our wildest dreams. The potential upside is inconceivably large, and while we cannot be absolutely confident we will find ourselves in the more positive futures, our decisions do effect the probability of their realization.*

> This is _less_ of a viable survival plan for your _planet_ if the first major failure of the bright-eyed youngsters kills _literally everyone_ before they can predictably get beaten about the head with the news that there were all sorts of unforeseen difficulties and reasons why things were hard.  You don't get any cynical old veterans, in this case, because everybody on Earth is dead.  Once you start to suspect you're in that situation, you have to do the Bayesian thing and update now to the view you will predictably update to later: realize you're in a situation of being that bright-eyed person who is going to encounter Unexpected Difficulties later and end up a cynical old veteran - or would be, except for the part where you'll be dead along with everyone else.  And become that cynical old veteran _right away,_ before reality whaps you upside the head in the form of everybody dying and you not getting to learn.  **Everyone else seems to feel that, so long as reality hasn't whapped them upside the head yet and smacked them down with the actual difficulties, they're free to go on living out the standard life-cycle and play out their role in the script and go on being bright-eyed youngsters; there's no cynical old veterans to warn them otherwise, after all, and there's no proof that everything won't go beautifully easy and fine,** _**given their bright-eyed total ignorance of what those later difficulties could be.**_

Yeah but if you're wrong, as you necessarily will be, because you cannot predict the future, you incur a risk by acting on assumptions about future events which by nature of causality you literally do not know.

This argument is deeply flawed in a basic probabilistic sense. *You are not the man from the future, and thus you cannot reason from subjectively inferred future information and treat it as though it is objectively certain past information*.

Glossing over the fact that you *literally cannot do this* and just assuming that you can is a recipe for disaster. 

> **It does not appear to me that the field of 'AI safety' is currently being remotely productive on tackling its enormous lethal problems.**

Agreed. A big part of this might perhaps be attributed partially to the fields philosophical monologues that fail to produce actionable solutions.

> These problems are in fact out of reach; the contemporary field of AI safety has been selected to contain people who go to work in that field anyways.  Almost all of them are there to tackle problems on which they can appear to succeed and publish a paper claiming success; if they can do that and get funded, why would they embark on a much more unpleasant project of trying something harder that they'll fail at, just so the human species can die with marginally more dignity?  This field is not making real progress and does not have a recognition function to distinguish real progress if it took place.  You could pump a billion dollars into it and it would produce mostly noise to drown out what little progress was being made elsewhere.

Ironic.

> **I figured this stuff out using the** [**null string**](https://twitter.com/ESYudkowsky/status/1500863629490544645) **as input,**

Read the thread. Don't understand the point.

> and frankly, I have a hard time myself feeling hopeful about getting real alignment work out of somebody who previously sat around waiting for somebody else to input a persuasive argument into them. 

No comment.

> This ability to "notice lethal difficulties without Eliezer Yudkowsky arguing you into noticing them" currently is an opaque piece of cognitive machinery to me, I do not know how to train it into others.

This is insanely pretentious.

> It probably relates to '[security mindset](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/)', and a mental motion where you refuse to play out scripts, and being able to operate in a field that's in a state of chaos.

No comment.

> "Geniuses" with nice legible accomplishments in fields with tight feedback loops where it's easy to determine which results are good or bad right away, and so validate that this person is a genius, are (a) people who might not be able to do equally great work away from tight feedback loops, (b) people who chose a field where their genius would be nicely legible even if that maybe wasn't the place where humanity most needed a genius, and (c) probably don't have the mysterious gears simply because they're _rare._  **You cannot just pay $5 million apiece to a bunch of legible geniuses from other fields and expect to get great alignment work out of them.**

No comment.

> They probably do not know where the real difficulties are, they probably do not understand what needs to be done, _they cannot tell the difference between good and bad work_, and the funders also can't tell without me standing over their shoulders evaluating everything, which I do not have the physical stamina to do.

I quite sure EY's approach increases existential risk, but I may be wrong. The fact that he is so sure of himself is suboptimal, because of course he may be wrong, and he has a lot more influence than me.

> I concede that real high-powered talents, especially if they're still in their 20s, genuinely interested, and have done their reading, are people who, yeah, fine, have higher probabilities of making core contributions than a random bloke off the street. But I'd have more hope - not significant hope, but _more_ hope - in separating the concerns of (a) credibly promising to pay big money retrospectively for good work to anyone who produces it, and (b) venturing prospective payments to somebody who is predicted to maybe produce good work later.

No comment.

> **Reading this document cannot make somebody a core alignment researcher**.  That requires, not the ability to read this document and nod along with it, but the ability to spontaneously write it from scratch without anybody else prompting you; that is what makes somebody a peer of its author.  It's guaranteed that some of my analysis is mistaken, though not necessarily in a hopeful direction.  The ability to do new basic work noticing and fixing those flaws is the same ability as the ability to write this document before I published it, which nobody apparently did, despite my having had other things to do than write this up for the last five years or so.  Some of that silence may, possibly, optimistically, be due to nobody else in this field having the ability to write things comprehensibly - such that somebody out there had the knowledge to write all of this themselves, if they could only have written it up, but they couldn't write, so didn't try.  I'm not particularly hopeful of this turning out to be true in real life, but I suppose it's one possible place for a "positive model violation" (miracle).  The fact that, twenty-one years into my entering this death game, seven years into other EAs noticing the death game, and two years into even normies starting to notice the death game, it is still Eliezer Yudkowsky writing up this list, says that humanity still has only one gamepiece that can do that.  I knew I did not actually have the physical stamina to be a star researcher, I tried really really hard to replace myself before my health deteriorated further, and yet here I am writing this.  That's not what surviving worlds look like.

Cut the pessimism and get off 4chan.

> **There's no plan.**

There are an abundance of plans. There's a lack of will and faith to execute these plans proactively because people that care are afraid that "we might fail". I've already stated how I feel about this.

> Surviving worlds, by this point, and in fact several decades earlier, have a plan for how to survive.  It is a written plan.  The plan is not secret.  In this non-surviving world, there are no candidate plans that do not immediately fall to Eliezer instantly pointing at the giant visible gaping holes in that plan. Or if you don't know who Eliezer is, you don't even realize you need a plan, because, like, how would a human being possibly realize that without Eliezer yelling at them?

I get this point, but it comes off as pretentious.

> It's not like people will yell at _themselves_ about prospective alignment difficulties, they don't have an _internal_ voice of caution.  So most organizations don't have plans, because I haven't taken the time to personally yell at them.  'Maybe we should have a plan' is deeper alignment mindset than they possess without me standing constantly on their shoulder as their personal angel pleading them into... continued noncompliance, in fact.

Time for a new strategy clearly.

> Relatively few are aware even that they should, to look better, produce a _pretend_ plan that can fool EAs too '[modest](https://equilibriabook.com/toc/)' to trust their own judgments about seemingly gaping holes in what serious-looking people apparently believe.

Lol.

> **This situation you see when you look around you is not what a surviving world looks like.**  The worlds of humanity that survive have plans.  They are not leaving to one tired guy with health problems the entire responsibility of pointing out real and lethal problems proactively.

Taking personal responsibility for saving the world is as exhausting as it is invigorating. I refer anyone who does this to the serenity prayer from earlier.

> Key people are taking internal and real responsibility for finding flaws in their own plans, instead of considering it their job to propose solutions and somebody else's job to prove those solutions wrong.  That world started trying to solve their important lethal problems earlier than this.  Half the people going into string theory shifted into AI alignment instead and made real progress there.  When people suggest a planetarily-lethal problem that might materialize later - there's a lot of people suggesting those, in the worlds destined to live, and they don't have a special status in the field, it's just what normal geniuses there do - they're met with either solution plans or a reason why that shouldn't happen, not an uncomfortable shrug and 'How can you be sure that will happen' / 'There's no way you could be sure of that now, we'll have to wait on experimental evidence.'

NC.

> A lot of those better worlds will die anyways.

Yep. Such is life. 

> It's a genuinely difficult problem, to solve something like that on your first try.  But they'll die with more dignity than this.

What a pessimistic article.










---

Another threat: we start creating intentionally misaligned AI's for experimental purposes in order to mitigate the risk of future rogue AI. This is an absolutely terrible idea, but it is a direct policy consequence of the worldview and fear espoused in this post.













%%

# Alignment Ideas

Each task should be assessed by a criteria that tries to determine whether 

# Research Proposals

A/B testing and shock testing LLM's (this has already been done I know) but even if companies don't make the models public, make the results of these tests public. Specifically, testing trillions of inputs and outputs, selectively modifying them, observing the changes in output, and building a "stimulus-response" networks for models, which attempt to show how the outputs change given the parameter count, hyper-parameter selection, training data size, context size and content. Then, attempt to build [[definition > sere]]-models which model how the dynamics of the model change when you vary the differing parameters.

[TODO: add multiplication test as example]





## Policy Proposals

Obviously nuking every chip fab in the world is untenable, and the arms race isn't going to stop. I get EY's point that this is only true because people believe it is true, but to break the game theoretical dilemma you would probably need some kind of cataclysmic event that induces a collective fear of artificial intelligence. I'm not proposing a false flag, by the way... I think there are alternative paths that are less reactionary, carry less risk, and are more likely to be beneficial to humanity.

1. Create deterrents *now* against the creation of counter-aligned systems.
	- Updating the nuclear first strike policy so that the US reserves the right to preemptively use nuclear weapons against any country identified as intentionally engaging in R&D to create counter-aligned systems *might* be a good first step.
	- If we did implement this policy, then it would be also a good idea to ensure that China is involved, and that they update their nuclear policy the same way as well. Ideally, we can start by reaching an agreement that neither the US, nor China, will ever create counter-aligned systems for any reason, and will use extreme force against any entity identified as attempting to do so. The reason I specify the US and China here is because these are the two states leading in AI research, eclipsing pretty much all other countries. Accordingly, in the short to medium term, this is where the risk is for the origination of such a system. 
		- We (US+China) can still engage in politically adversarial dynamics, espionage, and  perhaps (though I hope not) even war, etc. It should be clear, however, that the active deployment of a counter-aligned superintelligence would be orders of magnitude worse than global thermonuclear war, so nipping it in the bud and maintaining that policy with diplomacy and extreme force if necessary should be a priority.
			- The cat is out of the bag in respect to weakly aligned AI, and maybe out of the bag for disaligned AI. If it ever seems like counter-aligned AI is on the horizon, it probably wouldn't be completely unreasonable to buy a gun with a calibre large enough to eliminate the possibility your brain can be reconstructed by a superintelligent machine. I should note, I criticize EY for fear mongering in his piece, but the prior sentence might meet that definition too. I think the difference is that the vision he proposes implies a sort of helpless doomerism about the inevitability of humanities destruction. I think there's a good chance we'll be fine, and in a few thousand years we (or whatever we've become) will be living much better lives than we do today, with unlimited potential for further gains. There are some risks though in the interim period, but unlike EY I think we should be able to handle the medium tier ones like extinction, and we still have time to act to mitigate the really bad ones like condemning all living beings to an indefinite hell.



