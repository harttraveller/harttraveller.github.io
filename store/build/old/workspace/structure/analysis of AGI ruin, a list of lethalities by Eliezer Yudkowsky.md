[AGI Ruin: A List of Lethalities - LessWrong](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities)



> I have several times failed to write up a well-organized list of reasons why AGI will kill you.

- [[claim > AGI will destroy humanity]]

> People come in with different ideas about why AGI would be survivable, and want to hear different obviously key points addressed first.

- null

> Some fraction of those people are loudly upset with me if the obviously most important points aren't addressed immediately, and I address different points first instead.

- null

> Having failed to solve this problem in any good way, I now give up and solve it poorly with a poorly organized list of individual rants.  I'm not particularly happy with this list; the alternative was publishing nothing, and publishing this seems marginally more dignified.

- null

> Three points about the general subject matter of discussion here, numbered so as not to conflict with the list of lethalities:

- null

> I'm assuming you are already familiar with some basics, and already know what 'orthogonality' and 'instrumental convergence' are and why they're true.

- [[claim > the orthogonality thesis is true]]
- [[claim > instrumental convergence is true]]

> People occasionally claim to me that I need to stop fighting old wars here, because, those people claim to me, those wars have already been won within the important-according-to-them parts of the current audience.

- null

> I suppose it's at least true that none of the current major EA funders seem to be visibly in denial about orthogonality or instrumental convergence as such; so, fine. If you don't know what 'orthogonality' or 'instrumental convergence' are, or don't see for yourself why they're true, you need a different introduction than this one.

- null

> When I say that alignment is lethally difficult, I am not talking about ideal or perfect goals of 'provable' alignment, nor total alignment of superintelligences on exact human values, nor getting AIs to produce satisfactory arguments about moral dilemmas which sorta-reasonable humans disagree about, nor attaining an absolute certainty of an AI not killing everyone.  When I say that alignment is difficult, I mean that in practice, using the techniques we actually have, "please don't disassemble literally everyone with probability roughly 1" is an overly large ask that we are not on course to get.

- [[claim > there is a near 100% probability AI will destroy humanity]]

> So far as I'm concerned, if you can get a powerful AGI that carries out some pivotal superhuman engineering task, with a less than fifty percent change of killing more than one billion people, I'll take it. 

- 

> Even smaller chances of killing even fewer people would be a nice luxury, but if you can get as incredibly far as "less than roughly certain to kill everybody", then you can probably get down to under a 5% chance with only slightly more effort.  Practically all of the difficulty is in getting to "less than certainty of killing literally everyone".

- [[claim > there is a near 100% probability AI will destroy humanity]]

