Alternative titles for this article could include: Yudkosky is simultaneously too pessimistic, and not pessimistic enough.

His talk creates fear, which will create fear, which will cause people to act in fearful ways. For someone as intelligent as him it is amazing that he think scaring the bejeezus out of people will cause them to act and thus mitigate existential risk.

Let's not forget that before the trinity nuclear test, the scientists involved didn't really know whether or not it would set the atmosphere on fire and wipe out humanity.

- moving AI R&D from microsoft labs to CIA Labs, or perhaps just accelerating their merger...
- CIA stockpiling
	- the irony of saying: "be afraid everyone" is that - and to be clear EY is *far* more intelligent than me - is that the totality of your fear mongering will not make every average person feel fear 
- use ASI as deterrent
- mutually assured hell
- now lets consider all the previous times life advanced substantially
	- realistically, the precedent is that we are consumed. In fact, on a scale from -10 to +10 where the former is the worst case scenario, and the latter is the best case scenario, the one where every human drops dead simultaneously I think falls neatly in the middle at a 0, or perhaps a -1-ish. I think it would be *far* worse if some kind of super-intelligent organism took over, and had some kind of use for us that could be fulfilled via the industrialized mass farming of human beings. I see this as being pretty unlikely, at least in any way that draws immediate direct parallels to 
- The one effective deterrent, create a near superintelligence that could *feasibly* recursively improve its intelligence, bury it in a research lab somewhere underground with some people with some safety keys and buttons, and set it so if you country is attacked, the ASI is released and given the explicit goal to maximize the quantity and temporal duration of suffering for the people who attacked your country.
	- You can adapt this to prevent AGI takeoff as well. In the spirit of "How I learned to stop worrying and love the bomb" where such a similar doomsday machine is invoked, we might refer to this as a hell machine, because if successfully deployed it would both create hell, and be a hell of a lot worse than a doomsday machine.
	- Recurring dreams.
- We can also use decision theory and the simulation framework to force other peoples decisions.


The risk of fear mongering is:
1. All the people in their bubbles have to get closer to the edge, and this is scary, so they reach out for someone or something to protect them.
2. All of the people who have been dealing with this kind of existential fear for decades and are completely desensitized to it will think: "ah shit here we go again", reach out to do there job, and critically, they will use the tools of game theoretical deterrence 


Mention threads? Probably a good idea.

Furthermore, we have to be realistic, if the technology democratizes, eventually some really messed up individuals will get their hands on the technology. On the other hand, trying to surpress the technology will probably require nuclear war, or ironically the use of the technology itself.


## Recommendations

Basically just keep in mind that if we do end up going extinct, this is not by any stretch, the worst possible scenario. In fact, EY's scenario in which we all die at the same time instantly would be relatively merciful. 