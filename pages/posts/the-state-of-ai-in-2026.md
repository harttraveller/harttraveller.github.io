---
search:
  exclude: true
---

- llms not great at actually reasoning, don't appear to be actually intelligent
- representation problem unsolved
    - representation as network of information
        - extremely diverse/dynamic network, capable of representing physical geometry, abstract concepts, subjective feelings(?), temporal causal relations, etc
            - is there a single system for all these representations? feels like it as they are strongly interopable
                - or maybe there's multiple types of representations, but we have a strong adapter system for bridging between one representation and another form - like when thinking about a math problem in a spatial manner suddenly elucidates the answer
    - and specifically, the behind the scenes algorithm that can break down and recombine representations into new representations, and update them incredibly effectively when there is a delta between the perceptual input and the internal representation
    - [show abstract example of recombination of information networks here, from input networks, decomposition, recombination to new predicted network, and then alignment of new representation with reality based on perceived error]
- 3 groups, dist meme
    - agi tomorrow!
    - agi not coming
    - agi eventually coming
        - (1) hypemaxxing is useful ATM because it helps create awareness
        - (1) hypemaxxing is counter-productive because its setting up, or entrenching a boy-who-cried-wolf paradigm 
- when AGI does come, it will probably not be a compression/retrieval algorithm that simulates intelligence, but an actually alien kind of intelligence
    - so it's good that we're in a timeline where one of the biggest bumps in our progress towards AGI is a bunch of chinese rooms that try and simulate being human - that simulation is on some level intrinsically aligned without effort
- predictions re: timeline, or possibilities
    - [low P] transformer-esque and DNN scaling/harness hacks etc. will solve intelligence from a practical perspective
    - [med P] whatever intelligence is (I think representation and operator algo on representations) is a few very smart papers away from being solved, and we will be in for a wild ride when that happens, but whether it happens is a coin flip
    - [?] it is a very hard if not intractable problem
- do we end up with an intelligence explosion? not necessarily...
- would it be good if we ended up with an intelligence explosion? almost certainly not... (at the moment at least)